{
 "cells": [
  {
   "source": [
    "About image sizes\n",
    "https://ai.stackexchange.com/questions/6274/how-can-i-deal-with-images-of-variable-dimensions-when-doing-image-segmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision.utils\n",
    "\n",
    "import os,sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "import simulation\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(image, mask, alpha = False):\n",
    "    # preview side by side\n",
    "    if alpha == False:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        #fig.suptitle('Horizontally stacked subplots')\n",
    "        ax1.imshow(image.permute(1, 2, 0))\n",
    "        ax2.imshow(mask)\n",
    "    else:\n",
    "        # transform Tensor to PIL image\n",
    "        trans = transforms.ToPILImage()\n",
    "        image_pil=trans(image)\n",
    "        # mask is in palette mode so change to RGB to blend them together\n",
    "        mask_rgb=mask.convert('RGB')\n",
    "        #print(mask_rgb.mode)\n",
    "        blend_image = Image.blend(image_pil, mask_rgb, alpha=alpha)\n",
    "        plt.imshow(blend_image)\n",
    "    plt.show()\n",
    "\n",
    "def preview_tensor(tensor):\n",
    "    plt.imshow(tensor.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToOneHotMask(object):\n",
    "    \"\"\"Convert a ``PIL Image`` to numpy OneHotMask.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mask (PIL Image): Image to be converted to numpy onehot mask.\n",
    "        Returns:\n",
    "            Numpy array: Converted image.\n",
    "        \"\"\"\n",
    "        def mask2onehot(mask):\n",
    "            \"\"\"\n",
    "            Converts a segmentation mask (H,W) to (K,H,W) where the last dim is a one-hot encoding vector\n",
    "            0: background\n",
    "            [1 .. 20] interval: segmented objects, classes [Aeroplane, ..., Tvmonitor]\n",
    "            255: void category, used for border regions (5px) and to mask difficult objects\n",
    "            \"\"\"\n",
    "            classes = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,255]\n",
    "            _mask = [mask == i for i in classes]\n",
    "            return np.array(_mask).astype(np.uint8)\n",
    "\n",
    "        return mask2onehot(np.asarray(mask))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToEvenSize(object):\n",
    "    \"\"\"\n",
    "    Transforms the image so that HW\n",
    "    \"\"\"\n",
    "    def __call__(self, obj):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mask (PIL Image): Image to be converted to numpy onehot mask.\n",
    "        Returns:\n",
    "            Numpy array: Converted image.\n",
    "        \"\"\"\n",
    "        lst=list(obj.shape)\n",
    "        H, W = lst[0], lst[1]\n",
    "\n",
    "        if ((H%2 == 0) and (W%2 == 0)):\n",
    "            pass\n",
    "        else:\n",
    "            if H%2 == 0:\n",
    "                obj=np.delete(obj, -1, 1)\n",
    "            elif W%2 == 0:\n",
    "                obj=np.delete(obj, -1, 0)\n",
    "            else:             \n",
    "                obj=np.delete(obj, -1, 0)\n",
    "                obj=np.delete(obj, -1, 1)  \n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50, 100)\n[[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "w, h = 50,100\n",
    "\n",
    "img = np.zeros((w,h))\n",
    "#img = np.eye(10)\n",
    "#print(img)\n",
    "print(img.shape)\n",
    "\n",
    "\n",
    "while w%8!=0:\n",
    "    w-=1\n",
    "    print(w)\n",
    "# del columns\n",
    "\n",
    "row = 2\n",
    "img=img[:-row, :]\n",
    "col=1\n",
    "img=img[:,:-col]\n",
    "# del rows\n",
    "row = 0\n",
    "#img=np.delete(img, slice(-row,-1), 1)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToProperSize(img):\n",
    "    \"\"\"\n",
    "    Transforms the image so that HW\n",
    "    \"\"\"\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mask (PIL Image): Image to be converted to numpy onehot mask.\n",
    "        Returns:\n",
    "            Numpy array: Converted image.\n",
    "        \"\"\"\n",
    "        W, H = img.size\n",
    "\n",
    "        if ((H%8 == 0) and (W%8 == 0)):\n",
    "            pass\n",
    "        else:\n",
    "            if H%8 == 0:\n",
    "                img=np.delete(img, -1, 1)\n",
    "            elif W%8 == 0:\n",
    "                img=np.delete(img, -1, 0)\n",
    "            else:             \n",
    "                img=np.delete(img, -1, 0)\n",
    "                img=np.delete(img, -1, 1)  \n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using downloaded and verified file: input\\VOCtrainval_06-Nov-2007.tar\n",
      "Using downloaded and verified file: input\\VOCtrainval_06-Nov-2007.tar\n",
      "{'train': 209, 'val': 213}\n",
      "<class 'torch.Tensor'> torch.Size([3, 366, 500])\n",
      "<class 'numpy.ndarray'> 4026000\n"
     ]
    }
   ],
   "source": [
    "image_transform = transforms.Compose([\n",
    "    #transforms.CenterCrop((224,224)),\n",
    "    ToEvenSize(),\n",
    "    transforms.ToTensor()  \n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    #transforms.CenterCrop((224,224)),\n",
    "    ToEvenSize(),\n",
    "    ToOneHotMask()   \n",
    "])\n",
    "\n",
    "train_set = datasets.VOCSegmentation('input', year='2007', image_set = 'train', download = True, transform=image_transform, target_transform=mask_transform)\n",
    "val_set = datasets.VOCSegmentation('input', year='2007', image_set = 'val', download = True, transform=image_transform, target_transform=mask_transform)\n",
    "\n",
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "print(dataset_sizes)\n",
    "\n",
    "image, target = train_set[1]\n",
    "print(type(image), image.size())\n",
    "print(type(target), target.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'convert'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a2316577f9de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mpreview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-f36be2a145f3>\u001b[0m in \u001b[0;36mpreview\u001b[1;34m(image, mask, alpha)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mimage_pil\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;31m# mask is in palette mode so change to RGB to blend them together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mmask_rgb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;31m#print(mask_rgb.mode)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mblend_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_pil\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_rgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'convert'"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    image, mask = train_set[i]\n",
    "    preview(image, mask, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'torch.Tensor'> torch.Size([3, 224, 224])\n<class 'torch.Tensor'> <built-in method size of Tensor object at 0x000001BFFF928958>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'getpalette'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6f7d9ad11e2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# get mask palette\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mpalette\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpalette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# create onehot from mask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'getpalette'"
     ]
    }
   ],
   "source": [
    "# modified from https://www.programmersought.com/article/97425131352/\n",
    "# classes\n",
    "def mask2onehot(mask):\n",
    "    \"\"\"\n",
    "    Converts a segmentation mask (H,W) to (K,H,W) where the last dim is a one-hot encoding vector\n",
    "    0: background\n",
    "    [1 .. 20] interval: segmented objects, classes [Aeroplane, ..., Tvmonitor]\n",
    "    255: void category, used for border regions (5px) and to mask difficult objects\n",
    "    \"\"\"\n",
    "    classes = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,255]\n",
    "    _mask = [mask == i for i in classes]\n",
    "    return np.array(_mask).astype(np.uint8)\n",
    "\n",
    "def onehot2mask(onehot):\n",
    "    \"\"\"\n",
    "    Converts a mask (K, H, W) to (H,W)\n",
    "    \"\"\"\n",
    "    _mask = np.argmax(onehot, axis=0).astype(np.uint8)    \n",
    "    return np.where(_mask==21, 255, _mask)\n",
    "\n",
    "\n",
    "# get an image, mask pair\n",
    "image, mask = train_set[0]\n",
    "\n",
    "print(type(image), image.size())\n",
    "print(type(mask), mask.size)\n",
    "\n",
    "# get mask palette\n",
    "palette = mask.getpalette()\n",
    "\n",
    "# create onehot from mask\n",
    "onehot = mask2onehot(np.asarray(mask))\n",
    "print(type(onehot), onehot.shape)\n",
    "\n",
    "# create mask from onehot\n",
    "back = onehot2mask(onehot)\n",
    "print(type(back), back.shape)\n",
    "\n",
    "# check if the same\n",
    "print((back==mask).all())\n",
    "\n",
    "# Create PIL image from array\n",
    "mask_1 = Image.fromarray(back, 'P') # palette mode\n",
    "mask_1.putpalette(palette) # set the palette\n",
    "print(type(mask_1), mask_1.size)\n",
    "\n",
    "# preview image before and after\n",
    "preview(image,mask, 0.5)\n",
    "preview(image,mask_1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_pascal_label_colormap():\n",
    "#   \"\"\"Creates a label colormap used in PASCAL VOC segmentation benchmark.\n",
    "\n",
    "#   Returns:\n",
    "#     A Colormap for visualizing segmentation results.\n",
    "#   \"\"\"\n",
    "#   colormap = np.zeros((256, 3), dtype=int)\n",
    "#   ind = np.arange(256, dtype=int)\n",
    "\n",
    "#   for shift in reversed(range(8)):\n",
    "#     for channel in range(3):\n",
    "#       colormap[:, channel] |= ((ind >> channel) & 1) << shift\n",
    "#     ind >>= 3\n",
    "\n",
    "#   return colormap.flatten().tolist()\n",
    "# palette = create_pascal_label_colormap()\n",
    "\n",
    "# def pascal_palette():\n",
    "#     colors_dict = {\n",
    "#         0: [0, 0, 0],        # 0=background\n",
    "#         1: [128, 0, 0],      # 1=aeroplane\n",
    "#         2: [0, 128, 0],      # 2=bicycle\n",
    "#         3: [128, 128, 0],    # 3=bird\n",
    "#         4: [0, 0, 128],      # 4=boat\n",
    "#         5: [128, 0, 128],    # 5=bottle\n",
    "#         6: [0, 128, 128],    # 6=bus\n",
    "#         7: [128, 128, 128],  # 7=car\n",
    "#         8: [64, 0, 0],       # 8=cat\n",
    "#         9: [192, 0, 0],      # 9=chair\n",
    "#         10: [64, 128, 0],    # 10=cow\n",
    "#         11: [192, 128, 0],   # 11=diningtable\n",
    "#         12: [64, 0, 128],    # 12=dog\n",
    "#         13: [192, 0, 128],   # 13=horse\n",
    "#         14: [64, 128, 128],  # 14=motorbike\n",
    "#         15: [192, 128, 128], # 15=person\n",
    "#         16: [0, 64, 0],      # 16=potted plant\n",
    "#         17: [128, 64, 0],    # 17=sheep\n",
    "#         18: [0, 192, 0],     # 18=sofa\n",
    "#         19: [128, 192, 0],   # 19=train\n",
    "#         20: [0, 64, 128],    # 20=tv/monitor\n",
    "#         255: [224, 224, 192] # 255=void\n",
    "#         }\n",
    "#     palette = []\n",
    "#     for i in np.arange(256):\n",
    "#         if i in colors_dict:\n",
    "#             palette.extend(colors_dict[i])\n",
    "#         else:\n",
    "#             palette.extend([0, 0, 0])\n",
    "#     return palette\n",
    "\n",
    "# palette = pascal_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 3, 375, 500]) torch.Size([1, 22, 375, 500])\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of training data\n",
    "inputs, masks = next(iter(dataloaders['train']))\n",
    "print(inputs.shape, masks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs.shape, masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())\n",
    "\n",
    "plt.imshow(reverse_transform(inputs[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 224, 224]           1,792\n              ReLU-2         [-1, 64, 224, 224]               0\n            Conv2d-3         [-1, 64, 224, 224]          36,928\n              ReLU-4         [-1, 64, 224, 224]               0\n         MaxPool2d-5         [-1, 64, 112, 112]               0\n            Conv2d-6        [-1, 128, 112, 112]          73,856\n              ReLU-7        [-1, 128, 112, 112]               0\n            Conv2d-8        [-1, 128, 112, 112]         147,584\n              ReLU-9        [-1, 128, 112, 112]               0\n        MaxPool2d-10          [-1, 128, 56, 56]               0\n           Conv2d-11          [-1, 256, 56, 56]         295,168\n             ReLU-12          [-1, 256, 56, 56]               0\n           Conv2d-13          [-1, 256, 56, 56]         590,080\n             ReLU-14          [-1, 256, 56, 56]               0\n        MaxPool2d-15          [-1, 256, 28, 28]               0\n           Conv2d-16          [-1, 512, 28, 28]       1,180,160\n             ReLU-17          [-1, 512, 28, 28]               0\n           Conv2d-18          [-1, 512, 28, 28]       2,359,808\n             ReLU-19          [-1, 512, 28, 28]               0\n         Upsample-20          [-1, 512, 56, 56]               0\n           Conv2d-21          [-1, 256, 56, 56]       1,769,728\n             ReLU-22          [-1, 256, 56, 56]               0\n           Conv2d-23          [-1, 256, 56, 56]         590,080\n             ReLU-24          [-1, 256, 56, 56]               0\n         Upsample-25        [-1, 256, 112, 112]               0\n           Conv2d-26        [-1, 128, 112, 112]         442,496\n             ReLU-27        [-1, 128, 112, 112]               0\n           Conv2d-28        [-1, 128, 112, 112]         147,584\n             ReLU-29        [-1, 128, 112, 112]               0\n         Upsample-30        [-1, 128, 224, 224]               0\n           Conv2d-31         [-1, 64, 224, 224]         110,656\n             ReLU-32         [-1, 64, 224, 224]               0\n           Conv2d-33         [-1, 64, 224, 224]          36,928\n             ReLU-34         [-1, 64, 224, 224]               0\n           Conv2d-35         [-1, 22, 224, 224]           1,430\n================================================================\nTotal params: 7,784,278\nTrainable params: 7,784,278\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 460.14\nParams size (MB): 29.69\nEstimated Total Size (MB): 490.41\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_unet\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = pytorch_unet.UNet(22)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from loss import dice_loss\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "        \n",
    "    pred = F.sigmoid(pred)\n",
    "    dice = dice_loss(pred, target)\n",
    "    \n",
    "    loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "    \n",
    "    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))    \n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=1):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device,dtype=torch.float32)\n",
    "                labels = labels.to(device,dtype=torch.float32)\n",
    "                print(inputs.shape)  \n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\nEpoch 0/0\n----------\nLR 0.0001\ntorch.Size([1, 3, 500, 332])\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 124 and 125 in dimension 2 (The offending index is 1)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-56f72d5f42bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-064ad5a3d1e6>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;31m# track history if only in train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\evgen\\OneDrive - Imperial College London\\ML\\_Image_Segmentation_Capstone_Project\\pytorch_unet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdconv_up3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 124 and 125 in dimension 2 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 22\n",
    "\n",
    "model = pytorch_unet.UNet(num_class).to(device)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6, 192, 192)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAKvCAYAAABtZtkaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X+sbGd5H/rvUztwJUIFxPtavv5RG8shCml6AltOEIELBRJDURzCFbVVJU6DekAFqU1y1ZJQFW6vkKI2BCnKjclBWDZXiSGtQ7Fy3QYX0UCQKRwT1zEEg+0YcY6MvcERWElEYvu5f5zZYTjsHzN7ZvbM7PX5SFt75p21Zp61fR6/3/Puddaq7g4AAAzV31l2AQAAsEwCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIO2sEBcVVdV1b1VdV9VvWVRnwMAALOoRVyHuKrOSfKFJK9IcirJp5Nc292fm/uHAQDADBa1Qnxlkvu6+4Hu/usk709y9YI+CwAADuzcBb3vhUm+PPb8VJIf3m3jqnK7PIbsq929sewipnHeeef1pZdeuuwyYCnuvPPOtepZ/cqQTdqviwrE+6qq40mOL+vzYYV8adkFTGK8Zy+55JKcPHlyyRXBclTVyvesfoUzJu3XRZ0ycTrJxWPPLxqN/a3uPtHdm929uaAagDka79mNjbVZHINB0q8wnUUF4k8nuaKqLquqpyS5JsmtC/osAAA4sIWcMtHdj1fVm5P8QZJzktzQ3Z9dxGcBAMAsFnYOcXffluS2Rb0/AADMgzvVAQAwaAIxAACDJhADADBoAjEAAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIN24EBcVRdX1Uer6nNV9dmq+hej8bdX1emqumv09ar5lQsAAPN17gz7Pp7kF7v7M1X19CR3VtXto9fe1d2/Ont5AACwWAcOxN39UJKHRo8fq6o/TXLhvAoDAIDDMJdziKvq0iQ/lOR/jIbeXFV3V9UNVfXMeXwGAAAswsyBuKq+O8ktSf5ld38jyfVJLk9yLGdWkN+5y37Hq+pkVZ2ctQZg8cZ7dmtra9nlAHvQrzCdmQJxVX1XzoTh3+7u30uS7n64u5/o7ieTvCfJlTvt290nunuzuzdnqQE4HOM9u7GxsexygD3oV5jOLFeZqCTvTfKn3f1rY+MXjG32miT3HLw8AABYrFmuMvHCJD+d5E+q6q7R2C8nubaqjiXpJA8mecNMFQIAwALNcpWJP0pSO7x028HLAQCAw+VOdQAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgnTvrG1TVg0keS/JEkse7e7OqnpXkA0kuTfJgktd195/P+lkAADBv81ohfml3H+vuzdHztyT5SHdfkeQjo+cAALByFnXKxNVJbho9vinJTy7ocwAAYCbzCMSd5MNVdWdVHR+Nnd/dD40efyXJ+XP4HAAAmLuZzyFO8qPdfbqq/tckt1fV58df7O6uqj57p1F4Pn72OLCaxnv2kksuWXI1wF70K0xn5hXi7j49+v5Ikg8muTLJw1V1QZKMvj+yw34nuntz7LxjYIWN9+zGxsayywH2oF9hOjMF4qp6WlU9fftxkh9Lck+SW5NcN9rsuiQfmuVzAABgUWY9ZeL8JB+squ33+p3u/q9V9ekkv1tVr0/ypSSvm/FzAABgIWYKxN39QJJ/sMP415K8bJb3BgCAw+BOdQAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgnXvQHavqOUk+MDb07CT/NskzkvyzJFuj8V/u7tsOXCEAACzQgQNxd9+b5FiSVNU5SU4n+WCSf5rkXd39q3OpEAAAFmhep0y8LMn93f2lOb0fAAAcinkF4muS3Dz2/M1VdXdV3VBVz5zTZwAAwNzNHIir6ilJfiLJfxwNXZ/k8pw5neKhJO/cZb/jVXWyqk7OWgOweOM9u7W1tf8OwNLoV5jOPFaIX5nkM939cJJ098Pd/UR3P5nkPUmu3Gmn7j7R3ZvdvTmHGoAFG+/ZjY2NZZcD7EG/wnTmEYivzdjpElV1wdhrr0lyzxw+AwAAFuLAV5lIkqp6WpJXJHnD2PC/r6pjSTrJg2e9tjDdvetrVXUYJQBTeOyu3f8+/vRjTx5iJcB+9CtH3UyBuLv/Isn3nDX20zNVdLA69n1dKIbVsdfkuv26SRZWg35lCNb+TnX7heFptwMWa7/JddrtgMXRrwzFWv8JnjbkCsWwXNNOmiZZWB79ypD40wsAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAzaWgfiaW+24eYcsFzTXrzfxf5hefQrQ7LWgTiZPOQKw7AaJp00Ta6wfPqVoVj7QJzsH3aFYVgt+02eJldYHfqVITh32QXMi9AL68UkCutDv3LUHYkVYgAAOCiBGACAQROIAQAYNIGYuejuZZcATKF+69XLLgGYkH5dvIkCcVXdUFWPVNU9Y2PPqqrbq+qLo+/PHI1XVf16Vd1XVXdX1fMWVTyrRSiG9WKShfWhXxdr0qtM3JjkN5K8b2zsLUk+0t2/UlVvGT3/10lemeSK0dcPJ7l+9P1ImiYEDuFKGN09iONkfR2/5fTE25547YULrGQ11G+9Ov2G3192GbAj/frt9OviTLRC3N0fS/LoWcNXJ7lp9PimJD85Nv6+PuOTSZ5RVRfMo9hV0t1Tr4geZJ91NIRjZP0cv+X0VJPrQfdZR1aeWDX6dXf6dTFmOYf4/O5+aPT4K0nOHz2+MMmXx7Y7NRo7MmYNfEMIjEM4RtbHrJOkSRYOj37dn36dv7n8o7o+k36mSkBVdbyqTlbVyXnUcFjmFfSGEBiHcIxDMt6zW1tbyy5nYvOaHE2yrBP9ql+Zzix3qnu4qi7o7odGp0Q8Mho/neTise0uGo19m+4+keREklTVWiSnvQLeXufN7rbfEM63HcIxDsV4z25ubq5Fz+41Ke51vuFu+x2/5fRKn6f4gsvfMfG2F73woh3HK85RPAr06+r367bxvr3j/rdOta9ziudnlhXiW5NcN3p8XZIPjY3/zOhqEz+S5Otjp1asrd1CbVXtG/j22mbVV1G369vv+yTvAYdpt0nyxGsv3HeS3GsbK08wf0Pt17P/EvuCy9/xbV+T0K/zMell125OckeS51TVqap6fZJfSfKKqvpikpePnifJbUkeSHJfkvck+edzr/qQ7RWGp7GOoXi75v2+72eVj5GjZ6/JdRrrOsnOg0mWwzLUfp0k8G5vs98qsH6d3aRXmbi2uy/o7u/q7ou6+73d/bXufll3X9HdL+/uR0fbdne/qbsv7+6/391rdY7wpA56GsC6nT4wjxXis98LluGgvzpdh1+5LopJlmXRr99ipfhwuFPdPnYKcbOG2p32X9WwOK8V4m2repwcHTutBs06Se60/6quOs2bSZZF0q+TecHl75joXGH9enACMXua5wrx2e8JrAeTLCzfdih2+sRizHKViUGa1ykPVbUWwXDSFeJ1OxWE4ZjXr1BPvPbCtV9lGnfRJy7ecfzL73v3IVcC36Jf9/a35xTf78oS82aFmD1NukI8j5VjAGB/01xikckIxOxp2hViK8YAwLoRiNmTFWIA4KgTiNmTFWIA4KgTiNmTFWIAWD3OI54vgZg9WSEGAI46gXhK81r5XJcVVCvErLt5XXrpKF7CCVaNfmVZBGL2ZIUYAObvjvvfuuwSGCMQ72MRt1lexO2gF8UKMetmEbdtXcTtZQH9yuoQiA/ooIFv3YKiFWKOioNOsn71CodvKP16x/1vPdBK8UH3Y3e1CgGtqpZfxD52+zlNE/zm8R4cSXd29+ayi5jG5uZmnzx5ctll7Gm3iXGalaJ5vAdHT1WtVc/qV/06ZJP2qxXiCe0WWrt731XfvbYRhmExdpsEj99yet9VpL22MbnC/OlXlu3cZRewTqpq12B7kJV2YRgW68RrL9x1ojzIr1ZNrrA4+pVl2neFuKpuqKpHquqesbH/UFWfr6q7q+qDVfWM0filVfVXVXXX6Ovdiyx+GeYVYoVhOBzzmhRNrrB4+pVlmeSUiRuTXHXW2O1JfqC7fzDJF5L80thr93f3sdHXG+dT5mqZNcwKw3C4Zp0cTa5wePQry7DvKRPd/bGquvSssQ+PPf1kkv9jvmWtvu1QO82pEoIwLM/2JDnNr15NrLAc+pXDNo9ziH8uyQfGnl9WVX+c5BtJ/k13f3wOn7GyhFxYLyZNWB/6lcMyUyCuqrcmeTzJb4+GHkpySXd/raqen+Q/V9Vzu/sbO+x7PMnxWT4fODzjPXvJJZcsuRpgL/oVpnPgy65V1c8meXWSf9Kj8wa6+5vd/bXR4zuT3J/ke3fav7tPdPfmOl3LEYZsvGc3NjaWXQ6wB/0K0zlQIK6qq5L8qyQ/0d1/OTa+UVXnjB4/O8kVSR6YR6EAALAI+54yUVU3J3lJkvOq6lSSt+XMVSWemuT20Tm0nxxdUeLFSf5dVf1NkieTvLG7H11Q7QAAMLNJrjJx7Q7D791l21uS3DJrUQAAcFjcuhkAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYtH0DcVXdUFWPVNU9Y2Nvr6rTVXXX6OtVY6/9UlXdV1X3VtWPL6pwAACYh0lWiG9MctUO4+/q7mOjr9uSpKq+P8k1SZ472uc3q+qceRULAADztm8g7u6PJXl0wve7Osn7u/ub3f1nSe5LcuUM9QEAwELNcg7xm6vq7tEpFc8cjV2Y5Mtj25wajQEAwEo694D7XZ/k/07So+/vTPJz07xBVR1PcvyAn88R0N1T71NVC6iESYz37CWXXLLkaliGT73wpVPvc+UnPrqAStiPfkW/TudAK8Td/XB3P9HdTyZ5T751WsTpJBePbXrRaGyn9zjR3ZvdvXmQGoDDNd6zGxsbyy4H2IN+hekcKBBX1QVjT1+TZPsKFLcmuaaqnlpVlyW5IsmnZisRAAAWZ99TJqrq5iQvSXJeVZ1K8rYkL6mqYzlzysSDSd6QJN392ar63SSfS/J4kjd19xOLKZ1Vtn06hFMcYD3c9PEXJEmue9EdS64E2I9+nb99A3F3X7vD8Hv32P4dSd4xS1EAAHBY3KkOAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBB2/c6xHC27ZtuzHtbYDG2L+I/67bnvfzMlLHx30wdsCjz6NfrXnRHPvOCn8rz7vi9eZV15Pm/GkvnbnawHr768seTJP/o//r4kisB9nLTx1+Q593xv/zt8ys/8dElVrMeBGKmNkmAdetmWB2T3N7VrWBhNcytXz8xr4qGwTnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoO0biKvqhqp6pKruGRv7QFXdNfp6sKruGo1fWlV/NfbauxdZPAAAzGqS6xDfmOQ3krxve6C7//H246p6Z5Kvj21/f3cfm1eBAACwSPsG4u7+WFVdutNrdeauC69L8g/nWxbrzg05YL24IQesD/06f7OeQ/yiJA939xfHxi6rqj+uqj+sqhfN+P4AALBQs966+dokN489fyjJJd39tap6fpL/XFXP7e5vnL1jVR1PcnzGzwcOyXjPXnLJJUuuBtiLfoXpHHiFuKrOTfJTST6wPdbd3+zur40e35nk/iTfu9P+3X2iuze7e/OgNQCHZ7xnNzY2ll0OsAf9CtOZ5ZSJlyf5fHef2h6oqo2qOmf0+NlJrkjywGwlAgDA4kxy2bWbk9yR5DlVdaqqXj966Zp8++kSSfLiJHePLsP2n5K8sbsfnWfBAAAwT5NcZeLaXcZ/doexW5LcMntZAABwONypDgCAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEGr7l52DamqrSR/keSry65lDs6L41gl63Acf6+7N5ZdxDSq6rEk9y67jjlYhz8fk3Ach2utela/rhzHcbgm6tdzD6OS/XT3RlWd7O7NZdcyK8exWo7Kcayge4/Cz/Wo/PlwHOxDv64Qx7GanDIBAMCgCcQAAAzaKgXiE8suYE4cx2o5Ksexao7Kz9VxrJajchyr5qj8XB3Hajkqx5FkRf5RHQAALMsqrRADAMChE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQVtYIK6qq6rq3qq6r6resqjPAQCAWVR3z/9Nq85J8oUkr0hyKsmnk1zb3Z+b+4cBAMAMFrVCfGWS+7r7ge7+6yTvT3L1gj4LAAAObFGB+MIkXx57fmo0BgAAK+XcZX1wVR1Pcnz09PnLqgNWwFe7e2PZRexnvGef9rSnPf/7vu/7llwRLMedd9658j2rX+GMSft1UYH4dJKLx55fNBr7W919IsmJJKmq+Z/IDOvjS8suYBLjPbu5udknT55cckWwHFW18j2rX+GMSft1UadMfDrJFVV1WVU9Jck1SW5d0GcBAMCBLWSFuLsfr6o3J/mDJOckuaG7P7uIzwIAgFks7Bzi7r4tyW2Len8AAJgHd6oDAGDQBGIAAAZNIAYAYNAEYgAABk0gBgBg0ARiAAAGTSAGAGDQBGIAAAZNIAYAYNAEYgAABk0gBgBg0ARiAAAGTSAGAGDQBGIAAAZNIAYAYNAEYgAABu3AgbiqLq6qj1bV56rqs1X1L0bjb6+q01V11+jrVfMrFwAA5uvcGfZ9PMkvdvdnqurpSe6sqttHr72ru3919vIAAGCxDhyIu/uhJA+NHj9WVX+a5MJ5FQYAAIdhLucQV9WlSX4oyf8YDb25qu6uqhuq6pnz+AwAAFiEmQNxVX13kluS/Mvu/kaS65NcnuRYzqwgv3OX/Y5X1cmqOjlrDcDijffs1tbWsssB9qBfYTozBeKq+q6cCcO/3d2/lyTd/XB3P9HdTyZ5T5Ird9q3u09092Z3b85SA3A4xnt2Y2Nj2eUAe9CvMJ1ZrjJRSd6b5E+7+9fGxi8Y2+w1Se45eHkAALBYs1xl4oVJfjrJn1TVXaOxX05ybVUdS9JJHkzyhpkqBACABZrlKhN/lKR2eOm2g5cDAACHy53qAAAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEE7d9Y3qKoHkzyW5Ikkj3f3ZlU9K8kHklya5MEkr+vuP5/1swAAYN7mtUL80u4+1t2bo+dvSfKR7r4iyUdGzwEAYOUs6pSJq5PcNHp8U5KfXNDnHEh3L7sEYAr1W69edgnAhPQr62gegbiTfLiq7qyq46Ox87v7odHjryQ5fw6fM1dCMawXkyysD/3KuplHIP7R7n5eklcmeVNVvXj8xT6TPL8jfVbV8ao6WVUn51DDgQjFMLnxnt3a2lpODSZZmIh+henMHIi7+/To+yNJPpjkyiQPV9UFSTL6/sgO+53o7s2x846XQiiGyYz37MbGxtLqMMnC/vQrTGemQFxVT6uqp28/TvJjSe5JcmuS60abXZfkQ7N8zqIJxbBeTLKwPvQr62DWFeLzk/xRVf3PJJ9K8v91939N8itJXlFVX0zy8tHzlSYUw3oxycL60K+supkCcXc/0N3/YPT13O5+x2j8a939su6+ortf3t2PzqfcxRKKYb2YZGF96FdWmTvVnUUohvVikoX1oV9ZVQLxDoRiWC8mWVgf+pVVJBDvQiiG9WKShfWhX1k1AvEehGJYLyZZWB/6lVUiEO9DKIb1YpKF9aFfWRUC8QSEYlgvJllYH/qVVSAQT0gohvVikoX1oV9ZNoF4CkIxrBeTLKwP/coyCcRTEophvZhkYX3oV5ZFID4AoRjWi0kW1od+ZRkE4gMSimG9mGRhfehXDptAPAOhGNaLSRbWh37lMAnEMxKKYb2YZGF96FcOy7nLLmAZqmrZJQBT6Df8/rJLACakX1lHVogBABg0gRgAgEE78CkTVfWcJB8YG3p2kn+b5BlJ/lmSrdH4L3f3bQeuEAAAFujAgbi7701yLEmq6pwkp5N8MMk/TfKu7v7VuVQIAAALNK9TJl6W5P7u/tKc3g8AAA7FvALxNUluHnv+5qq6u6puqKpnzukzAABg7mYOxFX1lCQ/keQ/joauT3J5zpxO8VCSd+6y3/GqOllVJ2etAVi88Z7d2trafwdgafQrTGceK8SvTPKZ7n44Sbr74e5+orufTPKeJFfutFN3n+juze7enEMNwIKN9+zGxsayywH2oF9hOvMIxNdm7HSJqrpg7LXXJLlnDp8BAAALMdOd6qrqaUlekeQNY8P/vqqOJekkD571GgAArJSZAnF3/0WS7zlr7KdnqggAAA6RO9UBADBoAjEAAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIMmEAMAMGgCMQAAgzZRIK6qG6rqkaq6Z2zsWVV1e1V9cfT9maPxqqpfr6r7quruqnreoooHAIBZTbpCfGOSq84ae0uSj3T3FUk+MnqeJK9McsXo63iS62cvEwAAFmOiQNzdH0vy6FnDVye5afT4piQ/OTb+vj7jk0meUVUXzKNYAACYt1nOIT6/ux8aPf5KkvNHjy9M8uWx7U6NxgAAYOXM5R/VdXcn6Wn2qarjVXWyqk7OowZgscZ7dmtra9nlAHvQrzCdWQLxw9unQoy+PzIaP53k4rHtLhqNfZvuPtHdm929OUMNwCEZ79mNjY1llwPsQb/CdGYJxLcmuW70+LokHxob/5nR1SZ+JMnXx06tAACAlXLuJBtV1c1JXpLkvKo6leRtSX4lye9W1euTfCnJ60ab35bkVUnuS/KXSf7pnGsGAIC5mSgQd/e1u7z0sh227SRvmqUoAAA4LO5UBwDAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAM2rnLLgDmqbtTVXP7DizWCy5/x9ze64773zq39wK+01HuVyvEHCnbIXZe3wGAo2/fQFxVN1TVI1V1z9jYf6iqz1fV3VX1wap6xmj80qr6q6q6a/T17kUWD2fr7rl+BwCOvklWiG9MctVZY7cn+YHu/sEkX0jyS2Ov3d/dx0Zfb5xPmTAZK8QAwLT2DcTd/bEkj5419uHufnz09JNJLlpAbTA1K8QAwLTmcQ7xzyX5L2PPL6uqP66qP6yqF83h/WFiVogBgGnNFIir6q1JHk/y26Ohh5Jc0t0/lOQXkvxOVf3dXfY9XlUnq+rkLDXAOCvEizPes1tbW8suB9iDfoXpHDgQV9XPJnl1kn/So/TQ3d/s7q+NHt+Z5P4k37vT/t19ors3u3vzoDXA2awQL854z25sbCy7HGAP+hWmc6BAXFVXJflXSX6iu/9ybHyjqs4ZPX52kiuSPDCPQmESVogBgGnte2OOqro5yUuSnFdVp5K8LWeuKvHUJLePVtI+ObqixIuT/Luq+pskTyZ5Y3c/uuMbwwJYIQYAprVvIO7ua3cYfu8u296S5JZZi4KDcqc6AGBa7lTHkWKFGACYlkDMkeIcYgBgWgIxR4oVYgBgWgIxR4oVYgBgWgIxR4oVYgBgWgIxR4oVYgBgWgIxR4oVYgBgWgIxR4oVYgBgWvvemAPWiRViWC933P/WZZcATOgo96sVYgAABk0gBgBg0ARiAAAGTSAGAGDQBGIAAAZNIAYAYNAEYgAABm3fQFxVN1TVI1V1z9jY26vqdFXdNfp61dhrv1RV91XVvVX144sqHAAA5mGSFeIbk1y1w/i7uvvY6Ou2JKmq709yTZLnjvb5zao6Z17FcrR0tzvCwRq56eMvyE0ff8GyywAmoF+ns++d6rr7Y1V16YTvd3WS93f3N5P8WVXdl+TKJHccuMKBmiYouqsaLN/xW05PvO2J1164wEqA/ehXzjbLrZvfXFU/k+Rkkl/s7j9PcmGST45tc2o0xoQOsmK6vY9gDIdvmon17H1MtHC49Cu7Oeg/qrs+yeVJjiV5KMk7p32DqjpeVSer6uQBazhyZj19wOkHLNJ4z25tbS27nJVwkMl1nvvDbvTrd9Kv7OVAK8Td/fD246p6T5LfHz09neTisU0vGo3t9B4nkpwYvcegk9w8g6zVYhZlvGc3NzcH3bPznBitPrEI+vVb9CuTOFAgrqoLuvuh0dPXJNm+AsWtSX6nqn4tyf+W5Iokn5q5yiNsvzC8V7Dda9/uFophAfabXPeaKPfa9/gtp02yMGf6lUntG4ir6uYkL0lyXlWdSvK2JC+pqmNJOsmDSd6QJN392ar63SSfS/J4kjd19xOLKf1omyTMbm/jVAlYvkkmx+1t/OoVlku/crZJrjJx7Q7D791j+3ckeccsRR2WZV/JYbfPn/azqmrH97JKzFHzqRe+dOJtr/zER+f++btNjNOuFJ147YU7vpdVJ44S/co6cae6JZlXGN5vP6vHMB/zmlz3289qFMxOvzItgXiFzLqaazUYDtesq0NWl+Dw6Ff2IhAvwU6rtvMKszu9j1VimM1Oq0Dzmhx3eh+rTnBw+pWDEIgBABi0We5Ux5ytNTNaAAAZK0lEQVTM+1SH3f6R3aId9DOn3c+pISzbvH91uts/2lm0mz7+gkPZ77oX3XGgz4F50K/6dRIC8YRmuV4wcLjOe9WDeeyuvX8B9vRjTx5SNcB+xq9IceUnPvod/atfWTSBeE5c4mz6vxS4qx6LcN6rHpxou8fu+juDn2SnXQnaXmka6goSi7Xduw+847Js/KNvf02/6tdFE4jnSMCD5Zk0CI/bXoUa+kQLyzTeuxv/6Eu7bqdfWSSBGABYiu0wvFcQhsPgKhML4DJncLgOsjo8br/zjYHp7Xf3uYOGYf3KIgx6hXiR1+x1TjHM304T7LwmR+cowvyd3bPb/3hu1pVh/cq8+WvWCpj3irIValiseV9yyYX9GZLDPk1CvzIJgRgAOBTbK8bOGWbVCMRLsOhTNSb5PGByi7xd6yJvMwuraNFhWL9yEALxCpk1FDtVAg7XrJOsX73C4dGv7EUgXpLdVm3nfftjq8MwH7utAh10ktxtP6tNMDv9yrT2DcRVdUNVPVJV94yNfaCq7hp9PVhVd43GL62qvxp77d2LLH7dzSsUC8NwOOY1yZpcYfH0K9OY5LJrNyb5jSTv2x7o7n+8/biq3pnk62Pb39/dx+ZV4FBNctc7p0jA6tieNPeaJP3KFVaDfuVs+wbi7v5YVV2602t1Jq29Lsk/nG9Zw1FVewbbg4bedVgdXoca4WwnXnvhnhPlQSfRdVhtuu5Fdyy7BJiKfmVSs96Y40VJHu7uL46NXVZVf5zkG0n+TXd/fMbPOFT7BdRp3mfabQ/7c+EoePqxJ+dyc45pLvK/PRnOYwVpHSZWWDX6lXmbdRa5NsnNY88fSnJJd/9Qkl9I8jtV9Xd32rGqjlfVyao6OWMNczdrqDzo/sv6XJjEeM9ubW0tu5xvM+sdqw66/6yTo8mVRTnK/XpQ+pW91CSrkqNTJn6/u39gbOzcJKeTPL+7T+2y339P8n92956ht6pW7mTYWVZr5xFMp/l8QXjt3dndm8suYhqbm5t98uRq/V12llXieUzQ06w+mVjXW1WtVc+uYr8mB+9Z/co0Ju3XWU6ZeHmSz4+H4araSPJodz9RVc9OckWSB2b4jKU5yGkM8wymQi5MZ3uSnGaSnedKlUkTpjPt6U76lUWa5LJrNye5I8lzqupUVb1+9NI1+fbTJZLkxUnuHl2G7T8leWN3PzrPgg/bpMFUgIXVMOmkuaxf2wLfol9ZFZNcZeLaXcZ/doexW5LcMntZq0XYhfVi8oT1oV9ZBe5UBwDAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADFp197JrSFVtJfmLJF9ddi1zcF4cxypZh+P4e929sewiplFVjyW5d9l1zME6/PmYhOM4XGvVs/p15TiOwzVRv557GJXsp7s3qupkd28uu5ZZOY7VclSOYwXdexR+rkflz4fjYB/6dYU4jtXklAkAAAZNIAYAYNBWKRCfWHYBc+I4VstROY5Vc1R+ro5jtRyV41g1R+Xn6jhWy1E5jiQr8o/qAABgWVZphRgAAA6dQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAM2sICcVVdVVX3VtV9VfWWRX0OAADMorp7/m9adU6SLyR5RZJTST6d5Nru/tzcPwwAAGawqBXiK5Pc190PdPdfJ3l/kqsX9FkAAHBg5y7ofS9M8uWx56eS/PD4BlV1PMnx0dPnL6gOWAdf7e6NZRexn/GefdrTnvb87/u+71tyRbAcd95558r3rH6FMybt10UF4n1194kkJ5KkquZ/3gasjy8tu4BJjPfs5uZmnzx5cskVwXJU1cr3rH6FMybt10WdMnE6ycVjzy8ajQEAwEpZVCD+dJIrquqyqnpKkmuS3LqgzwIAgANbyCkT3f14Vb05yR8kOSfJDd392UV8FgAAzGJh5xB3921JblvU+wMAwDy4Ux0AAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIMmEAMAMGgCMQAAgyYQAwAwaAcOxFV1cVV9tKo+V1Wfrap/MRp/e1Wdrqq7Rl+vml+5AAAwX+fOsO/jSX6xuz9TVU9PcmdV3T567V3d/auzlwcAAIt14EDc3Q8leWj0+LGq+tMkF86rMAAAOAxzOYe4qi5N8kNJ/sdo6M1VdXdV3VBVz9xln+NVdbKqTs6jBmCxxnt2a2tr2eUAe9CvMJ2ZA3FVfXeSW5L8y+7+RpLrk1ye5FjOrCC/c6f9uvtEd2929+asNQCLN96zGxsbyy4H2IN+henMFIir6rtyJgz/dnf/XpJ098Pd/UR3P5nkPUmunL1MAABYjFmuMlFJ3pvkT7v718bGLxjb7DVJ7jl4eQAAsFizXGXihUl+OsmfVNVdo7FfTnJtVR1L0kkeTPKGmSoEAIAFmuUqE3+UpHZ46baDlwMAAIfLneoAABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQROIAQAYNIEYAIBBE4gBABg0gRgAgEETiAEAGDSBGACAQTt31jeoqgeTPJbkiSSPd/dmVT0ryQeSXJrkwSSv6+4/n/WzAABg3ua1QvzS7j7W3Zuj529J8pHuviLJR0bPAQBg5SzqlImrk9w0enxTkp9c0OcAAMBM5hGIO8mHq+rOqjo+Gju/ux8aPf5KkvPP3qmqjlfVyao6OYcagAUb79mtra1llwPsQb/CdOYRiH+0u5+X5JVJ3lRVLx5/sbs7Z0Jzzho/0d2bY6dZACtsvGc3NjaWXQ6wB/0K05k5EHf36dH3R5J8MMmVSR6uqguSZPT9kVk/BwAAFmGmQFxVT6uqp28/TvJjSe5JcmuS60abXZfkQ7N8DgAALMqsl107P8kHq2r7vX6nu/9rVX06ye9W1euTfCnJ62b8HACAI+UFl79j6n3uuP+tC6iEmQJxdz+Q5B/sMP61JC+b5b1Zru5OVc38HTgcB5lYz2aihcMxS79u76tf58ud6tjRdpid9TsAwKoTiNnRmYuDzP4dAGDVCcTsyAoxAKyueZwmxbcIxOzICjEAMBQCMTuyQgwADIVAzI6sEAMAQyEQsyMrxADAUAjE7MgKMQAwFAIxO7JCDAAMhUDMjqwQAwBDIRCzIyvEAMBQCMTsyAoxADAUAjE7skIMAAyFQMyOrBADwOq64/63LruEI0UgZkdWiAGAoRCI2ZEVYgBgKM496I5V9ZwkHxgbenaSf5vkGUn+WZKt0fgvd/dtB66QpbBCDOvFr09hfejX1XPgFeLuvre7j3X3sSTPT/KXST44evld268dlTBsxRPWS/3Wq5ddAjAh/cqyzeuUiZclub+7vzSn91tJQjGsF5MsrA/9yjLNKxBfk+Tmsedvrqq7q+qGqnrmTjtU1fGqOllVJ+dUw6EQihmq8Z7d2traf4cVYZJliPQrTGfmQFxVT0nyE0n+42jo+iSXJzmW5KEk79xpv+4+0d2b3b05aw2HTShmiMZ7dmNjY9nlTMUky9DoV5jOPFaIX5nkM939cJJ098Pd/UR3P5nkPUmunMNnrByhGNaLSRbWh37lsM0jEF+bsdMlquqCsddek+SeOXzGShKKYb2YZGF96FcO00yBuKqeluQVSX5vbPjfV9WfVNXdSV6a5Odn+YxVJxTDejHJwvrQrxyWmQJxd/9Fd39Pd399bOynu/vvd/cPdvdPdPdDs5e52oRiWC8mWVgf+pXD4E51cyIUw3oxycL60K8smkA8R0IxrBeTLKwP/coiCcRzJhTDejHJwvrQryyKQLwAQjGsF5MsrA/9yiIIxAsiFMN6McnC+tCvzJtAvEBCMawXkyysD/3KPAnECyYUw3oxycL60K/Mi0B8CIRiWC8mWVgf+pV5EIgPiVAM68UkC+tDvzIrgfgQCcWwXkyysD70K7M4d9kFrIuqWnYJwBT6Db+/7BKACelXls0KMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIM2USCuqhuq6pGqumds7FlVdXtVfXH0/Zmj8aqqX6+q+6rq7qp63qKKBwCAWU162bUbk/xGkveNjb0lyUe6+1eq6i2j5/86ySuTXDH6+uEk14++swR7XfvYpeRg9Tx21+7rFE8/9uQhVgJMYree1a/rZaJA3N0fq6pLzxq+OslLRo9vSvLfcyYQX53kfX0miX2yqp5RVRd090PzKJjvdNAbfnS3UAxLcPyW0zuO/9gzfy3JzydJfvxZ7/qO1x+76++YZOGQnd2vJ157YZK9//K6/bp+XR+znEN8/ljI/UqS80ePL0zy5bHtTo3Gvk1VHa+qk1V1coYaBm/Wu9+5ex6TGu/Zra2tZZeztvYOw9/yB4/+/I7b7TcJQ6Jf52Wnfj1+y+mJ+1C/ro+5/JcarQZPlay6+0R3b3b35jxqGKKdwmxVWfVlIcZ7dmNjY9nlrKWdJtcTr70wJ1574Y4rwruFYtiPfp3dXv3K0TPLrZsf3j4VoqouSPLIaPx0kovHtrtoNMYcnR2GhWBYbbv92nXcdigeD8J/8OjP7xiWgcWZpF85WmZZIb41yXWjx9cl+dDY+M+MrjbxI0m+7vzh+RKGYb1MO7meHYCtFMPhEYaHadLLrt2c5I4kz6mqU1X1+iS/kuQVVfXFJC8fPU+S25I8kOS+JO9J8s/nXjV/SxiG9TLp5GpVGJZPGB6OSa8yce0uL71sh207yZtmKQoAAA6Lf/64xqwOw3qZdrXJKjEsz1796nJqR49AfERNGpaFalgNk06wJmJYH/p1fQjER9h+YVcYhtWy3+RpcoXVoV+PFoF4jU1yU43t6xKfHX6FYTh8u92UY9zTjz35t1+/eP+X8+E//4V8+M9/weQKh2yaft1pnPUyy3WIAQAGTwBef1aI19ykt152i2ZYDZOsOk2zHbA40/Tr9hfrSSBeQ2ef7rBf2HUjD1ius/+1+n6TphsDwPLM2q+sJ4F4Te0Uis8OvjuNCcOwHDtNsmdPpDuNCcNw+PTr8DiHeI1V1Y4heK/tgeU58doLd5xU99oeWA79OiwC8ZrbKRTvtt062D6Ww653mnOs1+VnyWraaZLdbbt1cNPHX5Akue5Fdxzq537qhS+deNsrP/HRBVbCUaZf52Md+lUgPgIENFgv6zJ5Avp1KJxDDADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKDtG4ir6oaqeqSq7hkb+w9V9fmquruqPlhVzxiNX1pVf1VVd42+3r3I4gEAYFaTrBDfmOSqs8ZuT/ID3f2DSb6Q5JfGXru/u4+Nvt44nzIBAGAx9g3E3f2xJI+eNfbh7n589PSTSS5aQG0AALBw8ziH+OeS/Jex55dV1R9X1R9W1Yt226mqjlfVyao6OYcagAUb79mtra1llwPsQb/CdGrC2/5emuT3u/sHzhp/a5LNJD/V3V1VT03y3d39tap6fpL/nOS53f2Nfd5/8vvmsnamuS3yQRyBO/Xd2d2byy5iGpubm33ypL/LHlXbt3ddlMO+bey8VdVa9ax+Pdr0694m7dcDrxBX1c8meXWSf9KjxNPd3+zur40e35nk/iTfe9DPAACARTv3IDtV1VVJ/lWS/727/3JsfCPJo939RFU9O8kVSR6YS6WsrWlWcLdXk4/Aqi+srWlWhLZXp9Z9FQnWlX6dj30DcVXdnOQlSc6rqlNJ3pYzV5V4apLbR8Hlk6MrSrw4yb+rqr9J8mSSN3b3ozu+MQAArIB9A3F3X7vD8Ht32faWJLfMWhQAABwWd6oDAGDQBGIAAAZNIAYAYNAEYgAABk0gBgBg0ARiAAAGTSAGAGDQBGIAAAbtQLduhkVxy2ZYL24BC+tDv+7OCjEAAIMmEAMAMGgCMQAAgyYQAwAwaAIxAACDJhADADBoAjEAAIO2byCuqhuq6pGqumds7O1Vdbqq7hp9vWrstV+qqvuq6t6q+vFFFQ4AAPMwyQrxjUmu2mH8Xd19bPR1W5JU1fcnuSbJc0f7/GZVnTOvYgEAYN72DcTd/bEkj074flcneX93f7O7/yzJfUmunKE+AABYqFnOIX5zVd09OqXimaOxC5N8eWybU6Ox71BVx6vqZFWdnKEG4JCM9+zW1tayywH2oF9hOgcNxNcnuTzJsSQPJXnntG/Q3Se6e7O7Nw9YA3CIxnt2Y2Nj2eUAe9CvMJ0DBeLufri7n+juJ5O8J986LeJ0kovHNr1oNAYAACvpQIG4qi4Ye/qaJNtXoLg1yTVV9dSquizJFUk+NVuJAACwOOfut0FV3ZzkJUnOq6pTSd6W5CVVdSxJJ3kwyRuSpLs/W1W/m+RzSR5P8qbufmIxpQMAwOz2DcTdfe0Ow+/dY/t3JHnHLEUBAMBhcac6AAAGTSAGAGDQBGIAAAZNIAYAYNAEYgAABk0gBgBg0ARiAAAGTSAGAGDQBGIAAAZNIAYAYNAEYgAABk0gBgBg0ARiAAAGTSAGAGDQBGIAAAZNIAYAYND2DcRVdUNVPVJV94yNfaCq7hp9PVhVd43GL62qvxp77d2LLB4AAGZ17gTb3JjkN5K8b3ugu//x9uOqemeSr49tf393H5tXgQAAsEj7BuLu/lhVXbrTa1VVSV6X5B/OtywAADgcs55D/KIkD3f3F8fGLquqP66qP6yqF+22Y1Udr6qTVXVyxhqAQzDes1tbW8suB9iDfoXpzBqIr01y89jzh5Jc0t0/lOQXkvxOVf3dnXbs7hPdvdndmzPWAByC8Z7d2NhYdjnAHvQrTOfAgbiqzk3yU0k+sD3W3d/s7q+NHt+Z5P4k3ztrkQAAsCizrBC/PMnnu/vU9kBVbVTVOaPHz05yRZIHZisRAAAWZ5LLrt2c5I4kz6mqU1X1+tFL1+TbT5dIkhcnuXt0Gbb/lOSN3f3oPAsGAIB5muQqE9fuMv6zO4zdkuSW2csCAIDD4U51AAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADFp197JrSFVtJfmLJF9ddi1zcF4cxypZh+P4e929sewiplFVjyW5d9l1zME6/PmYhOM4XGvVs/p15TiOwzVRv557GJXsp7s3qupkd28uu5ZZOY7VclSOYwXdexR+rkflz4fjYB/6dYU4jtXklAkAAAZNIAYAYNBWKRCfWHYBc+I4VstROY5Vc1R+ro5jtRyV41g1R+Xn6jhWy1E5jiQr8o/qAABgWVZphRgAAA7d0gNxVV1VVfdW1X1V9ZZl1zONqnqwqv6kqu6qqpOjsWdV1e1V9cXR92cuu86zVdUNVfVIVd0zNrZj3XXGr4/++9xdVc9bXuXfbpfjeHtVnR79N7mrql419tovjY7j3qr68eVUvf707OHTs3r2oPTr4dOv69mvSw3EVXVOkv8nySuTfH+Sa6vq+5dZ0wG8tLuPjV165C1JPtLdVyT5yOj5qrkxyVVnje1W9yuTXDH6Op7k+kOqcRI35juPI0neNfpvcqy7b0uS0Z+ra5I8d7TPb47+/DEFPbs0N0bP6tkp6deluTH6de36ddkrxFcmua+7H+juv07y/iRXL7mmWV2d5KbR45uS/OQSa9lRd38syaNnDe9W99VJ3tdnfDLJM6rqgsOpdG+7HMdurk7y/u7+Znf/WZL7cubPH9PRs0ugZ/XsAenXJdCv69mvyw7EFyb58tjzU6OxddFJPlxVd1bV8dHY+d390OjxV5Kcv5zSprZb3ev43+jNo1893TD267R1PI5VtO4/Rz27mvTsYqz7z1C/rqYj2a/LDsTr7ke7+3k58yuPN1XVi8df7DOX8Fi7y3isa90j1ye5PMmxJA8leedyy2HF6NnVo2fZjX5dPUe2X5cdiE8nuXjs+UWjsbXQ3adH3x9J8sGc+fXAw9u/7hh9f2R5FU5lt7rX6r9Rdz/c3U9095NJ3pNv/cpmrY5jha31z1HPrh49u1Br/TPUr6vnKPfrsgPxp5NcUVWXVdVTcuaE7FuXXNNEquppVfX07cdJfizJPTlT/3Wjza5L8qHlVDi13eq+NcnPjP4l7I8k+frYr31WzlnnXr0mZ/6bJGeO45qqempVXZYz/4DhU4dd3xGgZ1eHnmU/+nV16NdV191L/UryqiRfSHJ/krcuu54p6n52kv85+vrsdu1Jvidn/gXpF5P8tyTPWnatO9R+c878quNvcuY8n9fvVneSypl/pXx/kj9Jsrns+vc5jv93VOfdOdOgF4xt/9bRcdyb5JXLrn9dv/TsUmrXs3r2oD9z/Xr4tevXNexXd6oDAGDQln3KBAAALJVADADAoAnEAAAMmkAMAMCgCcQAAAyaQAwAwKAJxAAADJpADADAoP3/1Wo52AeZZucAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x864 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prediction\n",
    "\n",
    "import math\n",
    "\n",
    "model.eval()   # Set model to evaluate mode\n",
    "\n",
    "test_dataset = SimDataset(3, transform = trans)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, shuffle=False, num_workers=0)\n",
    "        \n",
    "inputs, labels = next(iter(test_loader))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "\n",
    "# Change channel-order and make 3 channels for matplot\n",
    "input_images_rgb = [reverse_transform(x) for x in inputs.cpu()]\n",
    "\n",
    "# Map each channel (i.e. class) to each color\n",
    "target_masks_rgb = [helper.masks_to_colorimg(x) for x in labels.cpu().numpy()]\n",
    "pred_rgb = [helper.masks_to_colorimg(x) for x in pred]\n",
    "\n",
    "helper.plot_side_by_side([input_images_rgb, target_masks_rgb, pred_rgb])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}